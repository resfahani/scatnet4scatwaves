{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dokhtdor/.conda/envs/test_imag/lib/python3.12/site-packages/dask/dataframe/__init__.py:31: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "import scipy as sp\n",
    "\n",
    "#import betterplot\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scatseisnet import ScatteringNetwork\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA, PCA, KernelPCA, SparsePCA, IncrementalPCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from fastcluster import linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import umap\n",
    "# explained_variance_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from matplotlib import colors\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqmin = 1\n",
    "freqmax = 8\n",
    "sr = 40\n",
    "\n",
    "reduce_type = np.mean\n",
    "\n",
    "name_fig = 'figure05'\n",
    "\n",
    "name = f\"final_version/{name_fig}\"\n",
    "\n",
    "savepath = f\"/bettik/dokhtdor/projects/mirko_volcano_simulations/figures/{name}/\"\n",
    "\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "FILEPATH = \"/bettik/dokhtdor/projects/mirko_volcano_simulations/dataset/\"\n",
    "\n",
    "vs = np.load(FILEPATH + 'vs_array.npy') \n",
    "\n",
    "data_s1 = np.hstack([np.load(FILEPATH + \"z_r1.npy\")[:, np.newaxis,:], np.load(FILEPATH + \"x_r1.npy\")[:, np.newaxis,:]] )\n",
    "data_s2 = np.hstack([np.load(FILEPATH + \"z_r2.npy\")[:, np.newaxis,:], np.load(FILEPATH + \"x_r2.npy\")[:, np.newaxis,:]] )\n",
    "data_s3 = np.hstack([np.load(FILEPATH + \"z_r_s3.npy\")[:, np.newaxis,:], np.load(FILEPATH + \"x_r_s3.npy\")[:, np.newaxis,:]] )\n",
    "data_s4 = np.hstack([np.load(FILEPATH + \"z_r_s4.npy\")[:, np.newaxis,:], np.load(FILEPATH + \"x_r_s4.npy\")[:, np.newaxis,:]] )\n",
    "data_s5 = np.hstack([np.load(FILEPATH + \"z_r_s5.npy\")[:, np.newaxis,:], np.load(FILEPATH + \"x_r_s5.npy\")[:, np.newaxis,:]] )\n",
    "\n",
    "# Concatenating the sources \n",
    "data = np.concatenate([data_s1, data_s2, data_s3, data_s4, data_s5], axis = 1) #+ \n",
    "\n",
    "# Filtering the data\n",
    "#datafilter = filter(data[:, :], freqmin = freqmin, freqmax = freqmax, fs = 40, order = 2)\n",
    "\n",
    "\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "\n",
    "sos = butter(10, [freqmin, freqmax], 'bandpass', fs=sr, output='sos')\n",
    "datafilter = sosfiltfilt(sos, data[:,:,:], axis=-1)\n",
    "\n",
    "\n",
    "datafilter = data[:, : ,:]#[:, np.newaxis, :]\n",
    "\n",
    "#norm_two_channel  = np.sqrt(np.linalg.norm((data[:,:,:]), axis =2, keepdims=True)[:, 0, :]**2 + np.linalg.norm((data[:,:,:]), axis =2, keepdims=True)[:, 0, :]**2 )\n",
    "\n",
    "# Normalizing the data\n",
    "\n",
    "#data = datafilter\n",
    "\n",
    "data = datafilter[:,:,::1] / np.linalg.norm(abs(datafilter[:, :, :]), ord = 2, axis = -1, keepdims=True)\n",
    "#data = datafilter[:,:,::1] / np.max(abs(datafilter[:, :, :]),  axis = -1, keepdims=True)\n",
    "\n",
    "#data  = data[:,:,::1] = data/ np.max(abs(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ComplexMorletBank.__init__() got an unexpected keyword argument 'taper_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m samples_per_segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(segment_duration_seconds \u001b[38;5;241m*\u001b[39m sampling_rate_hertz)\n\u001b[1;32m      8\u001b[0m bank_keyword_arguments \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moctaves\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaper_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m0.\u001b[39m},\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moctaves\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m7\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaper_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m0.\u001b[39m},\n\u001b[1;32m     11\u001b[0m      )\n\u001b[0;32m---> 14\u001b[0m network \u001b[38;5;241m=\u001b[39m ScatteringNetwork(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m*\u001b[39mbank_keyword_arguments,\n\u001b[1;32m     16\u001b[0m     bins\u001b[38;5;241m=\u001b[39msamples_per_segment,\n\u001b[1;32m     17\u001b[0m     sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate_hertz,\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, bank \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(network\u001b[38;5;241m.\u001b[39mbanks):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/bettik/PROJECTS/pr-f-image/dokhtdor/codes/scatseisnet/scatseisnet/network.py:100\u001b[0m, in \u001b[0;36mScatteringNetwork.__init__\u001b[0;34m(self, bins, sampling_rate, verbose, taper_alpha, kymatio_filterbankbanks, *layer_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbanks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m         ComplexMorletBank_new(bins, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m layer_kwargs\n\u001b[1;32m     97\u001b[0m     ]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbanks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 100\u001b[0m         ComplexMorletBank(bins, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m layer_kwargs\n\u001b[1;32m    102\u001b[0m     ]\n",
      "\u001b[0;31mTypeError\u001b[0m: ComplexMorletBank.__init__() got an unexpected keyword argument 'taper_alpha'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "segment_duration_seconds = data.shape[-1]/sr\n",
    "\n",
    "sampling_rate_hertz = sr\n",
    "\n",
    "samples_per_segment = int(segment_duration_seconds * sampling_rate_hertz)\n",
    "\n",
    "\n",
    "bank_keyword_arguments = (\n",
    "    {\"octaves\": 4 , \"resolution\": 8, \"quality\": 2, \"taper_alpha\" : 0.},\n",
    "    {\"octaves\": 7 , \"resolution\": 4, \"quality\": 3, \"taper_alpha\" : 0.},\n",
    "     )\n",
    "\n",
    "\n",
    "network = ScatteringNetwork(\n",
    "    *bank_keyword_arguments,\n",
    "    bins=samples_per_segment,\n",
    "    sampling_rate=sampling_rate_hertz,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "for i, bank in enumerate(network.banks):\n",
    "    print(f\"Bank {i}\")\n",
    "    print(bank)\n",
    "    for j in range(bank.wavelets.shape[0]):\n",
    "        network.banks[i].wavelets[j] = network.banks[i].wavelets[j] / np.sqrt((np.abs(network.banks[i].wavelets[j])**2).sum())\n",
    "\n",
    "network.banks[0].spectra = np.fft.fft(network.banks[0].wavelets)\n",
    "network.banks[1].spectra = np.fft.fft(network.banks[1].wavelets)\n",
    "\n",
    "\n",
    "\n",
    "print(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(network.banks[i].wavelets[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop over network layers\n",
    "#plt.figure(figsize=(10,5))\n",
    "\n",
    "#cc = low_pass_filter(2000, sigma = 0.2)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(7,5),  )\n",
    "\n",
    "\n",
    "for i, bank in enumerate(network.banks):\n",
    "\n",
    "    # Create axes (left for temporal, right for spectral domain)\n",
    "    \n",
    "    ii = 0\n",
    "    # Show each wavelet\n",
    "    for wavelet, spectrum, ratio in zip(\n",
    "        bank.wavelets[:], bank.spectra[:], bank.ratios[:]\n",
    "    ):\n",
    "\n",
    "        # Time domain\n",
    "        ax[i,0].plot(bank.times, wavelet.real + ratio, c= 'k', lw = 0.5)\n",
    "        #ax[i,0].plot(bank.times, wavelet.imag + ratio, c='r', lw = 0.5)\n",
    "\n",
    "        # Spectral domain (log of amplitude)\n",
    "        ax[i,1].plot(bank.frequencies, np.log(np.abs(spectrum) + 1) + ratio, c = 'k', lw = 0.5)\n",
    "        #ax[i,2].plot(bank.frequencies, np.log(np.abs(spectrum*cc[1000:]) + 1) + ratio, c = 'b', lw = 0.5)\n",
    "        \n",
    "    #ax[0,1].plot(bank.frequencies, np.log(np.abs(cc[1000:]) + 1) -0.25, c = 'r', lw = 0.5)\n",
    "\n",
    "    # Limit view to three times the temporal width of largest wavelet\n",
    "    width_max = 2 * bank.widths.max()\n",
    "\n",
    "    # Labels\n",
    "    \n",
    "    \n",
    "    ax[i,0].set_ylabel(\"Octaves (base 2 log)\")\n",
    "    ax[i,0].set_xlabel(\"Time (seconds)\")\n",
    "    ax[i,0].set_xlim(-width_max, width_max)\n",
    "    ax[i,0].grid()\n",
    "    ax[i,1].set_xscale(\"log\")\n",
    "    ax[i,1].set_xlabel(\"Frequency (Hz)\")\n",
    "    ax[i,1].grid()\n",
    "\n",
    "    #ax[i, 0].set_xlim(0.1, 50)\n",
    "    #  \n",
    "    #ax[i,2].set_xscale(\"log\")\n",
    "    #ax[i,2].set_xlabel(\"Frequency (Hz)\")\n",
    "    #ax[i,2].grid()\n",
    "    #ax[i, 1].set_xlim(0.1, 50)\n",
    "    \n",
    "    ii = ii+1\n",
    "\n",
    "for axn in ax.flat:\n",
    "    axn.spines['top'].set_visible(False)\n",
    "    axn.spines['right'].set_visible(False)\n",
    "    #ax.spines['bottom'].set_visible(False)\n",
    "    #ax.spines['left'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "#fig.savefig(savepath + f'filterbank.pdf', transparent=True, dpi = 300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time = np.arange(0, data.shape[-1], 1) / sr\n",
    "indxvs = np.arange(120*65).reshape(120, 65)\n",
    "\n",
    "indx = [15, 35, 55, 75, 95]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize = (4, 3))\n",
    "\n",
    "axs[0].plot(time, data[indxvs[75, 30], 0, :].T,lw = 0.75, c = 'k')\n",
    "\n",
    "axs[1].plot(time, data[indxvs[75, 30], 1, :].T,lw = 0.75, c = 'k')\n",
    "\n",
    "\n",
    "ax = axs[0]\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "#ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xlabel('Time (s)')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "#ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xlabel('Time (s)')\n",
    "\n",
    "fig.tight_layout()\n",
    "#fig.savefig(savepath + f'seismograms.pdf', transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [data[i,:,:] for i in indxvs[indx, 30]]\n",
    "\n",
    "\n",
    "sc1 = network.transform(segments,  reduce_type= None )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0, data.shape[-1], 1) / sr\n",
    "indxvs = np.arange(120*65).reshape(120, 65)\n",
    "\n",
    "indx = [15, 35, 55, 75, 95]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reader_coeff(data):\n",
    "    sc = [data['order_1'], data['order_2']]\n",
    "\n",
    "    #sc = [scm2['order_1'], scm2['order_2']]\n",
    "    for i in range(len(network.banks[0].centers)):\n",
    "        \n",
    "        # if network.banks[0].centers[i]>=10:\n",
    "\n",
    "        #      sc[0][:,:, i] = np.nan\n",
    "        #      sc[1][:,:, i,:] = np.nan\n",
    "\n",
    "        n = network.banks[0].centers[i] <= network.banks[1].centers  # Find the center of the wavelet in the second layer that is closest to the center of the wavelet in the first layer\n",
    "        sc[1][:,:, i, n] = np.nan\n",
    "    \n",
    "    return sc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scm1 = np.load('/bettik/dokhtdor/projects/mirko_volcano_simulations/coefficients/sc_normalized_l2_wavelet_unnormalized_median.npz')\n",
    "scm2 = np.load('/bettik/dokhtdor/projects/mirko_volcano_simulations/coefficients/sc_unnormalized_wavelet_unnormalized.npz')\n",
    "scm3 = np.load('/bettik/dokhtdor/projects/mirko_volcano_simulations/coefficients/sc_normalized_wavelet_unnormalized_coda.npz')\n",
    "#scm = np.load('/bettik/dokhtdor/projects/mirko_volcano_simulations/coefficients/sc_max_normalized_l2_wavelet_unnormalized_median.npz')\n",
    "\n",
    "\n",
    "scm_unnorm = reader_coeff(scm2)\n",
    "scm_coda = reader_coeff(scm3)\n",
    "scm_norm = reader_coeff(scm1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc = scm_unnorm\n",
    "\n",
    "\n",
    "gs = {\"hspace\": 0.4, \"wspace\": 0.1, \"height_ratios\": [1,1.5, 3]}\n",
    "fig, axs = plt.subplots(\n",
    "        3, 3, figsize=(8, 7), sharex=\"row\", sharey=\"row\", gridspec_kw=gs\n",
    "    )\n",
    "\n",
    "\n",
    "offset_x = 30\n",
    "\n",
    "trc = [35, 55, 80]\n",
    "\n",
    "ax = axs[0].reshape(-1)\n",
    "\n",
    "for i, ind in enumerate(trc):\n",
    "\n",
    "    ax[i].set_title(f\"Vs =  {np.ceil(vs.reshape(-1)[indxvs[ind, offset_x]])} m/s\")\n",
    "    ax[i].plot(np.arange(len(data[0,0,:]))/40, data[indxvs[ind, offset_x],0,:], lw = 0.5,  c = '#5770db')\n",
    "    ax[i].set_xlabel('Time (s)')\n",
    "    ax[i].set_xlim(0, 37)\n",
    "\n",
    "    \n",
    "ax = axs[1].reshape(-1)\n",
    "\n",
    "\n",
    "for i, ind in enumerate(trc):\n",
    "    ss = sc[0][indxvs[ind, offset_x]][0]\n",
    "    sss = ss #- np.mean(sc[0][:, 0, :], axis = 0)\n",
    "    sss = abs(sss)\n",
    "    \n",
    "    ax[i].plot(network.banks[0].centers, (sss), lw = 2,  c = '#db5f57')\n",
    "    #ax[i].set_ylim(0, 0.1)\n",
    "    #ax[i].plot(network.banks[0].centers, (sc2[0][indxvs[ind, offset_x]][0]), lw = 2,  c = 'k')\n",
    "\n",
    "    #ax[i].set_xlabel('1st order \\n frequency (Hz)')\n",
    "    ax[i].set_xlim(0., 10)\n",
    "\n",
    "ax[0].set_ylabel(\"Scattering \\n coefficients (order 1)\")\n",
    "            \n",
    "ax = axs[2].reshape(-1)\n",
    "\n",
    "for i, ind in enumerate(trc):\n",
    "\n",
    "    ss = (sc[1][indxvs[ind, offset_x]][0])\n",
    "    #ss[np.isnan(ss)] = 0 \n",
    "    sss = ss #- np.mean(sc[1][:, 0, : ,:], axis = 0) \n",
    "    sss = abs(sss)\n",
    "    img = ax[i].pcolormesh(network.banks[0].centers, network.banks[1].centers, ((sss.T)*1000), cmap = 'RdYlBu_r', rasterized=True ) \n",
    "    ax[i].set_xscale(\"log\")\n",
    "    ax[i].set_yscale(\"log\")\n",
    "    ax[1].set_xlim(network.banks[0].centers.min(), 10)\n",
    "    ax[1].set_ylim(network.banks[1].centers.min(), 10)\n",
    "\n",
    "    ax[i].grid(True)\n",
    "\n",
    "    ax[i].set_xlabel('1st order \\n central frequency (Hz)')\n",
    "\n",
    "    cb = plt.colorbar(img, ax=ax[i], orientation=\"horizontal\", pad=0.35)\n",
    "    cb.set_label(\"Scattering coefficients (order 2)\")\n",
    "    if i != 1:\n",
    "        cb.ax.set_visible(False)\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('2nd order \\n central frequency (Hz)')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + \"Coefficients\" + \".png\", bbox_inches=\"tight\", dpi=300, transparent=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizing(scattering_coefficients, sil = 5):\n",
    "    \n",
    "    ss = scattering_coefficients.copy()\n",
    "    \n",
    "    c1 = ss[0].copy()\n",
    "    c2 = ss[1].copy()\n",
    "\n",
    "    c2hat = c2/ (np.nanmedian(c1, -1) + sil + 0* np.nanmax(c1, -1) )[:,:,np.newaxis , np.newaxis] \n",
    "\n",
    "    c2hat[np.isnan(scattering_coefficients[1])] == np.nan\n",
    "    \n",
    "    return [c1, c2hat]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, normalize, PowerTransformer, QuantileTransformer, StandardScaler\n",
    "\n",
    "hfreq = 0\n",
    "\n",
    "def process__(scc, indx = None):\n",
    "    \n",
    "    #sc = normalizing(scc, sil = 1e-10)\n",
    "    sc = scc.copy()\n",
    "\n",
    "    if indx == None:\n",
    "        indx = np.arange(sc[0].shape[1])\n",
    "        order_1 = sc[0][:, ::2, hfreq:]\n",
    "        order_2 = sc[1][:, ::2, hfreq:, :]\n",
    "\n",
    "    elif indx == 'avg':\n",
    "        order_1 = np.nanmedian(sc[0][:, ::2, hfreq:], axis = 1)\n",
    "        order_2 = np.nanmedian(sc[1][:, ::2, hfreq:, :], axis = 1)\n",
    "\n",
    "    else:\n",
    "        indx = indx\n",
    "        order_1 = sc[0][:, indx, hfreq:]\n",
    "        order_2 = sc[1][:, indx, hfreq:, :]\n",
    "\n",
    "    #print(order_1)\n",
    "\n",
    "    #scc = normalizing(sc, sil = 1e-1)\n",
    "\n",
    "    order_1 = order_1.reshape(order_1.shape[0] , -1)\n",
    "    order_2 = order_2.reshape(order_2.shape[0], -1)\n",
    "    \n",
    "    coeff = np.hstack((order_1, order_2))\n",
    "\n",
    "    coeff =  np.log((coeff[:, :]) + 1e-2)# * np.nanmax(coeff) )\n",
    "\n",
    "    coef = np.nan_to_num(coeff, 0)\n",
    "\n",
    "    coef = MinMaxScaler(feature_range = (-1, 1)).fit_transform(coef)\n",
    "\n",
    "    #coef  = StandardScaler().fit_transform(coef)\n",
    "    \n",
    "    #coef = QuantileTransformer(\n",
    "    #        output_distribution=\"uniform\", random_state=42\n",
    "    #    ).fit_transform(coef)\n",
    "\n",
    "    return coef\n",
    "\n",
    "\n",
    "#scm_norm = reader_coeff(scm1)\n",
    "#scm_unnorm = reader_coeff(scm2)\n",
    "#scm_coda = reader_coeff(scm3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network.banks[0].centers, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Reduceing(model, data, n):\n",
    "\n",
    "    reducing_model = model(n_components= n , random_state = 0,)# max_iter= 1000, tol = 1e-20)\n",
    "    features = reducing_model.fit_transform(data)\n",
    "    iv  = reducing_model.inverse_transform(features)\n",
    "\n",
    "    varaince = explained_variance_score(data, iv)\n",
    "\n",
    "    return features, varaince\n",
    "\n",
    "from string import ascii_uppercase\n",
    "    \n",
    "#dd = MinMaxScaler(feature_range = (-1, 1)).fit_transform()\n",
    "\n",
    "def heir_cluster(features, threshold = 1, CLUSTERS = 5):\n",
    "    \n",
    "    linkage_matrix = linkage(features, \"ward\", metric = 'cityblock')    \n",
    "\n",
    "    META_THRESHOLD = threshold * linkage_matrix[:, 2].max()\n",
    "    # Find meta cluster\n",
    "\n",
    "    clusters =  fcluster(linkage_matrix, CLUSTERS, criterion='maxclust')\n",
    "    \n",
    "    \n",
    "    meta_clusters  = fcluster(linkage_matrix, META_THRESHOLD, criterion=\"distance\")\n",
    "    \n",
    "    cluster_indexes = np.arange(CLUSTERS) +1\n",
    "    \n",
    "    correspondance = np.zeros_like(cluster_indexes)\n",
    "    for i, cluster in enumerate(cluster_indexes):\n",
    "        correspondance[i] = np.unique(meta_clusters[clusters == cluster])[0]\n",
    "\n",
    "    meta_clusters_labels, meta_clusters_counts = np.unique(\n",
    "        correspondance, return_counts=True\n",
    "    )\n",
    "    labels = []\n",
    "    for n in meta_clusters_counts:\n",
    "        # If the cluster contains more than one sub-cluster, number them from 1 to n\n",
    "        if n > 1:\n",
    "            labels.extend(list(range(1, n + 1)))\n",
    "\n",
    "        # If the cluster contains only one sub-cluster, leave it without number\n",
    "        else:\n",
    "            labels.append(\"\")\n",
    "\n",
    "\n",
    "    labels = [\n",
    "        f\"{ascii_uppercase[c - 1]}{l}\" for c, l in zip(correspondance, labels)\n",
    "    ]\n",
    "\n",
    "    colors = []\n",
    "    shift = 1\n",
    "    for label in labels:\n",
    "        if len(label) > 1:\n",
    "            colors.append(f\"C{ascii_uppercase.index(label[:1]) + shift}\")\n",
    "        else:\n",
    "            shift -= 1\n",
    "            colors.append(\"C0\")\n",
    "    \n",
    "    #colors  = sns.color_palette(\"hls\", np.unique(meta_clusters).shape[0])\n",
    "\n",
    "    return linkage_matrix, clusters, colors\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def dendo(link, CLUSTERS, ax, threshold = 0.3):\n",
    "\n",
    "    with plt.rc_context({\"lines.linewidth\": 1}):\n",
    "        \n",
    "        dendrogram(link ,\n",
    "                CLUSTERS,\n",
    "                orientation=\"top\",\n",
    "                truncate_mode=\"lastp\",\n",
    "                ax = ax,\n",
    "                color_threshold = threshold * link[:, 2].max(),\n",
    "                above_threshold_color=\"k\",\n",
    "                no_labels=True,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "coef_4 = process__(scm_unnorm, indx = 4)\n",
    "coef_avg = process__(scm_unnorm, indx = 'avg')\n",
    "coef_all = process__(scm_unnorm, indx = None)\n",
    "coef_2 = process__(scm_norm, indx = None)\n",
    "\n",
    "\n",
    "mdl = FastICA\n",
    "\n",
    "\n",
    "d = np.arange(1, 20, 1)\n",
    "\n",
    "variance_4 = []\n",
    "ariance_avg = []\n",
    "ariance_all = []\n",
    "ariance_norm = []\n",
    "\n",
    "for i in d:\n",
    "\n",
    "    features_1, variance_1 = Reduceing(mdl, coef_4, i)\n",
    "    features_2, variance_2 = Reduceing(mdl, coef_avg, i)\n",
    "    features_3, variance_3 = Reduceing(mdl, coef_all, i)\n",
    "    features_4, variance__4 = Reduceing(mdl, coef_2, i)\n",
    "\n",
    "    variance_4.append(variance_1)\n",
    "    ariance_avg.append(variance_2)\n",
    "    ariance_all.append(variance_3)\n",
    "    ariance_norm.append(variance__4)\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax.spines['top'].set_visible(False)\n",
    "#ax.spines['right'].set_visible(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 4))\n",
    "ax.plot(d, variance_4, 'o-', label = ' R3', c = \"#db5f57\")\n",
    "ax.plot(d, ariance_avg, 'o-', label = 'avg. of all stations', c = '#57db94')\n",
    "ax.plot(d, ariance_all, 'o-', label = 'Concat. of all stations', c = '#5784db')\n",
    "ax.plot(d, ariance_all, 'o-', label = 'Concat. of all stations', c = '#5784db')\n",
    "ax.plot(d, ariance_norm, 'o-', label = 'Normalized seismograms', c = '#c957db')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Explained variance')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend( fontsize = 9 , ncol = 2)\n",
    "ax.vlines(3, 0.,1.1, linestyle = '--', color = 'k')\n",
    "\n",
    "ax.set_ylim(0.3, 0.95)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim([0, 15])\n",
    "ax.grid(alpha = 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(savepath + 'explained_variance.pdf', dpi = 300, transparent=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coef_4 = process__(scm_unnorm, indx = 4)\n",
    "#coef_avg = process__(scm_unnorm, indx = 'avg')\n",
    "coef_all = process__(scm_unnorm, indx = None)\n",
    "coef_2 = process__(scm_norm, indx = None)\n",
    "\n",
    "\n",
    "mdl = FastICA\n",
    "\n",
    "NCOMPT = 4\n",
    "\n",
    "features__4, variance_4 = Reduceing(mdl, coef_4, NCOMPT)\n",
    "#features_avg, variance_avg = Reduceing(mdl, coef_avg, NCOMPT)\n",
    "features_all, variance_all = Reduceing(mdl, coef_all, NCOMPT)\n",
    "\n",
    "features__1, variance_1 = Reduceing(mdl, coef_2, NCOMPT)\n",
    "\n",
    "variance_4, variance_all, variance_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = {\"hspace\": 0.2, \"wspace\": 0.3}\n",
    "\n",
    "fig, axs = plt.subplots(NCOMPT, 3, figsize = (7 , 16), gridspec_kw=gs)\n",
    "axs = axs.T\n",
    "\n",
    "vmax = 3\n",
    "vmin = -3\n",
    "CMAP = 'RdYlBu_r'\n",
    "ax = axs[0]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "\n",
    "    im1 = ax[i].imshow((features__4[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "    divider = make_axes_locatable(ax[i])\n",
    "    if i == 0:\n",
    "       ax[0].set_title(f'Station R3 \\n Explained var.: {variance_4:.2f} \\n Feature {i+1}')\n",
    "    else:\n",
    "        ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "# ax = axs[1]\n",
    "\n",
    "# for i in range(NCOMPT):\n",
    "#     im1 = ax[i].imshow((features_avg[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "#     if i == 0:\n",
    "#         ax[0].set_title(f'Avg. of all stations \\n Explained var.: {variance_avg:.2f} \\n Feature {i+1}')\n",
    "#     else:\n",
    "#         ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "# divider = make_axes_locatable(ax[i])\n",
    "# cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "# cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "\n",
    "#ax[0].set_title(f'Concat. of all stations \\n Explained var.: {variance_all:.2f}')\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "    im1 = ax[i].imshow((features_all[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[0].set_title(f' Concat. of all stations \\n Explained var.: {variance_all:.2f}  \\n Feature {i+1}')\n",
    "    else:\n",
    "        ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "divider = make_axes_locatable(ax[i])\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "\n",
    "\n",
    "#ax[0].set_title(f'Concat. of all stations \\n Explained var.: {variance_all:.2f}')\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "    im1 = ax[i].imshow((features__1[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[0].set_title(f' Normlized seismograms \\n Explained var.: {variance_1:.2f}  \\n Feature {i+1}')\n",
    "    else:\n",
    "        ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "divider = make_axes_locatable(ax[i])\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + 'ICA_components.pdf', dpi = 300, transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = {\"hspace\": 0.2, \"wspace\": 0.3}\n",
    "\n",
    "fig, axs = plt.subplots(NCOMPT, 3, figsize = (7 , 16), gridspec_kw=gs)\n",
    "axs = axs.T\n",
    "\n",
    "vmax = 3\n",
    "vmin = -3\n",
    "CMAP = 'RdYlBu_r'\n",
    "ax = axs[0]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "\n",
    "    im1 = ax[i].imshow((features__4[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "    divider = make_axes_locatable(ax[i])\n",
    "    if i == 0:\n",
    "       ax[0].set_title(f'Station R3 \\n Explained var.: {variance_4:.2f} \\n Feature {i+1}')\n",
    "    else:\n",
    "        ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "# ax = axs[1]\n",
    "\n",
    "# for i in range(NCOMPT):\n",
    "#     im1 = ax[i].imshow((features_avg[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "#     if i == 0:\n",
    "#         ax[0].set_title(f'Avg. of all stations \\n Explained var.: {variance_avg:.2f} \\n Feature {i+1}')\n",
    "#     else:\n",
    "#         ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "# divider = make_axes_locatable(ax[i])\n",
    "# cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "# cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "\n",
    "#ax[0].set_title(f'Concat. of all stations \\n Explained var.: {variance_all:.2f}')\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "    im1 = ax[i].imshow((features_all[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[0].set_title(f' Concat. of all stations \\n Explained var.: {variance_all:.2f}  \\n Feature {i+1}')\n",
    "    else:\n",
    "        ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "divider = make_axes_locatable(ax[i])\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "\n",
    "\n",
    "#ax[0].set_title(f'Concat. of all stations \\n Explained var.: {variance_all:.2f}')\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "    im1 = ax[i].imshow((features__1[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax, rasterized=True)\n",
    "\n",
    "    if i == 0:\n",
    "        ax[0].set_title(f' Normlized seismograms \\n Explained var.: {variance_1:.2f}  \\n Feature {i+1}')\n",
    "    else:\n",
    "        ax[i].set_title(f'Feature {i+1}')\n",
    "\n",
    "divider = make_axes_locatable(ax[i])\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'Absolute values ')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + 'ICA_components.pdf', dpi = 300, transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = 50\n",
    "threshold = 0.4\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "range_n_clusters = [2,3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "sil2 = []\n",
    "\n",
    "featureforsill = features_all\n",
    "#featureforsill = features__1\n",
    "\n",
    "for i in range_n_clusters:\n",
    "\n",
    "    #X, variance_norm = Reduceing(mdl, coef_all, i)\n",
    "\n",
    "    linkage_1, clusters_1, colors_1 = heir_cluster(featureforsill, threshold = threshold, CLUSTERS = i)\n",
    "\n",
    "    print(np.unique(clusters_1)) \n",
    "    #cluster_labels = clustering_norm\n",
    "\n",
    "    silhouette_avg = silhouette_score(featureforsill, clusters_1 )\n",
    "\n",
    "    sil2.append(silhouette_avg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range_n_clusters, sil2, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cmap2 = sns.color_palette(\"hls\", 7)\n",
    "\n",
    "print(cmap2.as_hex())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "linkage_1, clusters_1, colors_1 = heir_cluster(featureforsill, threshold = threshold, CLUSTERS = 4)\n",
    "silhouette_avg = silhouette_score(featureforsill, clusters_1 )\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,  figsize = (8, 3))\n",
    "n_clusters = len(np.unique(clusters_1))+1\n",
    "\n",
    "\n",
    "axs[0].plot(np.asarray(range_n_clusters), sil2, 'o-', c ='#5770db')\n",
    "axs[0].set_xlabel('Number of clusters')\n",
    "axs[0].set_ylabel('Silhouette average score')\n",
    "axs[0].vlines(n_clusters-1, 0.0, 0.8, linestyle = '--', color = '#db5f57')\n",
    "i#axs[0].set_ylim(0.1, 0.4)\n",
    "axs[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "axs[0].spines['top'].set_visible(False)\n",
    "axs[0].spines['right'].set_visible(False)\n",
    "\n",
    "sample_silhouette_values = silhouette_samples(featureforsill, clusters_1)\n",
    "y_lower = 10\n",
    "axs[0].set_title('a.', loc = 'left', fontsize = 10, fontweight = 'bold')\n",
    "\n",
    "ax1 = axs[1]\n",
    "ax1.set_title('b.', loc = 'left', fontsize = 10, fontweight = 'bold')\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[clusters_1 == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "    \n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=cmap2.as_hex()[i],\n",
    "        edgecolor=cmap2.as_hex()[i],\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    #ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "#ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "ax1.axvline(x=silhouette_avg, color=\"#db5f57\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax1.set_xlim(-0.1, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(savepath + 'silhouette_score.pdf', dpi = 300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = 100\n",
    "threshold = 0.4\n",
    "\n",
    "linkage_1, clusters_1, colors_1 = heir_cluster(features__4, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "#linkage_2, clusters_2, colors_2 = heir_cluster(features_avg, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "linkage_3, clusters_3, colors_3 = heir_cluster(features_all, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "linkage_4, clusters_4, colors_4 = heir_cluster(features__1, threshold = threshold, CLUSTERS = CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(linkage_1[-10:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = dendrogram(linkage_1 ,\n",
    "                19,\n",
    "                truncate_mode=\"lastp\",\n",
    "                orientation=\"top\",\n",
    "                color_threshold =  100, \n",
    "                # above_threshold_color=\"k\",\n",
    "                no_labels=True,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs = {\"hspace\": 0.02 , \"wspace\": 0.4, \"height_ratios\": [1.,3, 1., 3]}\n",
    "fig, axss = plt.subplots(\n",
    "        4, 3, figsize=(10, 12),  gridspec_kw=gs,\n",
    "    )\n",
    "\n",
    "axss.reshape(-1)[6].set_axis_off()\n",
    "axss.reshape(-1)[9].set_axis_off()\n",
    "\n",
    "\n",
    "axss = axss.T\n",
    "\n",
    "axs = axss[0]\n",
    "\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "im1 = axs[1].imshow(vs.reshape(120, 65), cmap = 'Spectral', aspect = 'auto')\n",
    "#divider = make_axes_locatable(axs[0])\n",
    "divider = make_axes_locatable(axs[1])\n",
    "cax1 = divider.append_axes('top', size='5%', pad=0.3, )\n",
    "#cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, )\n",
    "cax1.set_axis_off()\n",
    "axs[1].set_title('a.\\n ', loc = 'left', fontsize = 12, fontweight=\"bold\")\n",
    "\n",
    "axs[1].set_xlabel('Offset')\n",
    "axs[1].xaxis.set_label_position('top') \n",
    "axs[1].xaxis.tick_top()\n",
    "\n",
    "\n",
    "cbar.set_label('Vs (m/s)', labelpad=-40, y=1.05, rotation=0)\n",
    "\n",
    "\n",
    "ll = ['b', 'd', 'c', 'e', 'f']\n",
    "i = 0\n",
    "for axs, link, clus, colls in zip(axss[1:,:].reshape(4, 2), [linkage_1, linkage_3, linkage_4 ], [clusters_1,clusters_3, clusters_4], [ colors_1, colors_3, colors_4]):\n",
    "\n",
    "    ax = axs[0]\n",
    "    dendo(link, CLUSTERS, ax, threshold = threshold)\n",
    "    ax.set_ylim(0, link[:, 2].max() * 2)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    #plt.vlines(META_THRESHOLD, 0, 1000, colors='r', linestyles='dashed')\n",
    "    ax = axs[1]\n",
    "    ax.set_title(ll[i]+'. \\n \\n \\n \\n \\n', loc = 'left', fontsize = 12, fontweight=\"bold\")\n",
    "    i = i+1\n",
    "\n",
    "        \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax1 = divider.append_axes('top', size='5%', pad=0.3, )\n",
    "    \n",
    "\n",
    "    cmap = ListedColormap(colls)\n",
    "\n",
    "    im1 = ax.imshow(clus.reshape(120, 65),cmap = cmap, aspect='auto', rasterized=True)\n",
    "    #plt.colorbar(im, ax = cax1)\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35)\n",
    "    cbar.set_ticklabels([])\n",
    "\n",
    "    #ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(savepath + 'clustering_all.pdf', dpi = 300, transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scm_norm = reader_coeff(scm1)\n",
    "#scm_unnorm = reader_coeff(scm2)\n",
    "#scm_coda = reader_coeff(scm3)\n",
    "\n",
    "\n",
    "coef_1 = process__(scm_unnorm, indx = None)\n",
    "coef_2 = process__(scm_norm, indx = None)\n",
    "\n",
    "#coef_2  = RobustScaler().fit_transform(coef_2.T).T\n",
    "\n",
    "coef_3 = process__(scm_coda, indx = None)\n",
    "\n",
    "\n",
    "\n",
    "mdl = FastICA\n",
    "\n",
    "NCOMPT = 2\n",
    "\n",
    "\n",
    "features__1, variance_1 = Reduceing(mdl, coef_2, NCOMPT)\n",
    "features__2, variance_2 = Reduceing(mdl, coef_3, NCOMPT)\n",
    "\n",
    "features__3, variance_3 = Reduceing(mdl, coef_3, NCOMPT)\n",
    "\n",
    "variance_1, variance_2, variance_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = {\"hspace\": 0.2, \"wspace\": 0.4}\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (5.6, 4), gridspec_kw=gs)\n",
    "axs = axs.T\n",
    "\n",
    "vmax = 2.5\n",
    "vmin = 0\n",
    "CMAP = 'Spectral'\n",
    "ax = axs\n",
    "\n",
    "\n",
    "ax[0].set_title(f'Unnormalized data \\n (all stations) \\n Explained var.: {variance_1:.2f}')\n",
    "\n",
    "for i in range(2):\n",
    "\n",
    "    im1 = ax[i].imshow(abs(features__1[:, i].reshape(120, 65)), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax)\n",
    "\n",
    "divider = make_axes_locatable(ax[i])\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'ICA components {i+1}')\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "\n",
    "\n",
    "#ax[0].set_title(f'Late-coda normalization \\n (all stations) \\n Explained var.: {variance_2:.2f}')\n",
    "\n",
    "\n",
    "#for i in range(2):\n",
    "#    im1 = ax[i].imshow(features__2[:, i].reshape(120, 65), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax)\n",
    "\n",
    "#divider = make_axes_locatable(ax[i])\n",
    "#cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "#cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'ICA components {i+1}')\n",
    "\n",
    "\n",
    "# ax = axs[2]\n",
    "\n",
    "# ax[0].set_title(f'Concat. of all sources \\n Explained var.: {variance_3:.2f}')\n",
    "\n",
    "# for i in range(2):\n",
    "#     im1 = ax[i].imshow(features_3[:, i].reshape(120, 65), cmap = CMAP, aspect = 'auto', vmin = vmin, vmax = vmax)\n",
    "#     divider = make_axes_locatable(ax[i])\n",
    "#     cax1 = divider.append_axes('bottom', size='5%', pad=0.3, )\n",
    "#     cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, label = f'ICA components {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + 'ICA_components_normalized.pdf', dpi = 300, transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = 50\n",
    "threshold = 0.28\n",
    "\n",
    "linkage_1, clusters_1, colors_1 = heir_cluster(features__1, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "linkage_2, clusters_2, colors_2 = heir_cluster(features__2, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "gs = {\"hspace\": 0.01, \"wspace\": 0.4, \"height_ratios\": [0.5,3]}\n",
    "fig, axss = plt.subplots(\n",
    "        2, 2, figsize=(5, 4.5),  gridspec_kw=gs\n",
    "    )\n",
    "\n",
    "\n",
    "axss = axss.T\n",
    "\n",
    "\n",
    "axs = axss[0]\n",
    "axs[0].set_axis_off()\n",
    "im1 = axs[1].imshow(vs.reshape(120, 65), cmap = 'Spectral', aspect = 'auto')\n",
    "divider = make_axes_locatable(axs[1])\n",
    "cax1 = divider.append_axes('top', size='5%', pad=0.3, )\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35, )\n",
    "\n",
    "cbar.set_label('Vs (m/s)', labelpad=-40, y=1.05, rotation=0)\n",
    "\n",
    "\n",
    "\n",
    "for axs, link, clus, colls in zip(axss[1:,:], [linkage_1 ], [clusters_1], [ colors_1]):\n",
    "\n",
    "    ax = axs[0]\n",
    "    dendo(link, CLUSTERS, ax, threshold = threshold)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    #plt.vlines(META_THRESHOLD, 0, 1000, colors='r', linestyles='dashed')\n",
    "\n",
    "    ax = axs[1]\n",
    "\n",
    "    \n",
    "        \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax1 = divider.append_axes('top', size='5%', pad=0.3, )\n",
    "    \n",
    "\n",
    "    cmap = ListedColormap(colls)\n",
    "\n",
    "    im1 = ax.imshow(clus.reshape(120, 65),cmap = cmap, aspect='auto', rasterized=True)\n",
    "    #plt.colorbar(im, ax = cax1)\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35)\n",
    "    cbar.set_ticklabels([])\n",
    "\n",
    "    #ax.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(savepath + 'clustering_normalized.pdf', dpi = 300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(range_n_clusters, sil, 'o-')\n",
    "plt.plot(range_n_clusters, sil2, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "range_n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs = {\"hspace\": 0.01, \"wspace\": 0.2, \"height_ratios\": [0.5,3]}\n",
    "fig, axss = plt.subplots(\n",
    "        2, 2, figsize=(6, 6),  gridspec_kw=gs\n",
    "    )\n",
    "\n",
    "axss = axss.T\n",
    "\n",
    "for axs, link, clus, colls in zip(axss, [linkage_unnorm, linkage_norm ], [clusters_unnorm, clusters_norm], [ colors_unnorm, colors_norm]):\n",
    "\n",
    "    ax = axs[0]\n",
    "    dendo(link, CLUSTERS, ax, threshold = 0.3)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    #plt.vlines(META_THRESHOLD, 0, 1000, colors='r', linestyles='dashed')\n",
    "\n",
    "    ax = axs[1]\n",
    "\n",
    "\n",
    "        \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax1 = divider.append_axes('top', size='5%', pad=0.3, )\n",
    "    \n",
    "\n",
    "\n",
    "    cmap = ListedColormap(colls)\n",
    "\n",
    "    im1 = ax.imshow(clus.reshape(120, 65),cmap = cmap, aspect='auto', rasterized=True)\n",
    "    #plt.colorbar(im, ax = cax1)\n",
    "\n",
    "    cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.35)\n",
    "    cbar.set_ticklabels([])\n",
    "\n",
    "    #ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_unnorm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = np.arange(2, 14, 1)\n",
    "\n",
    "variance_coda_all = []\n",
    "for i in d:\n",
    "\n",
    "    features_coda, variance_coda = Reduceing(mdl, coefficients_norm, i)\n",
    "    variance_coda_all.append(variance_coda)\n",
    "\n",
    "#clustering_coda = clustering(features_coda, threshold = threshold, CLUSTERS = CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(d, variance_coda_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,NCOMPT , figsize = (NCOMPT*2, 5))\n",
    "\n",
    "for i in range(NCOMPT):\n",
    "    \n",
    "    ax = axs[i]\n",
    "    im = ax.imshow(features_unorm[:, i].reshape(120, 65), aspect = 'auto', cmap = 'RdYlBu_r')\n",
    "    plt.colorbar(im, ax = ax, orientation = 'horizontal', label = 'ICA 1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = None#0.25\n",
    "\n",
    "CLUSTERS = 8\n",
    "clustering_norm = clustering(features_norm, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "clustering_unorm = clustering(features_unorm, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "clustering_coda = clustering(features_coda, threshold = threshold, CLUSTERS = CLUSTERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "\n",
    "range_n_clusters = [ 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "X = features\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3), dpi = 300)\n",
    "    #fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    #clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    #cluster_labels = clusterer.fit_predict(X)\n",
    "    X = features_norm\n",
    "\n",
    "    clustering_norm = clustering(X, threshold = threshold, CLUSTERS = n_clusters)\n",
    "\n",
    "    cluster_labels = clustering_norm\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    #centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # ax2.scatter(\n",
    "    #     centers[:, 0],\n",
    "    #     centers[:, 1],\n",
    "    #     marker=\"o\",\n",
    "    #     c=\"white\",\n",
    "    #     alpha=1,\n",
    "    #     s=200,\n",
    "    #     edgecolor=\"k\",\n",
    "    # )\n",
    "\n",
    "    # for i, c in enumerate(centers):\n",
    "    #     ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    # ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    # ax2.set_xlabel(\"Feature space (1st)\")\n",
    "    # ax2.set_ylabel(\"Feature space (2nd)\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.savefig(savepath+f'silhouette_{i}.png', bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "gs = {\"hspace\": 0.4, \"wspace\": 0.2}\n",
    "fig, axs = plt.subplots(1, 2,  dpi = 300, figsize = (6, 4), sharex = True, sharey = True, gridspec_kw=gs)\n",
    "\n",
    "for i, modelcluster in enumerate(zip([clustering_unorm, clustering_norm], ['Unnormalized data', 'Normalized data'])):\n",
    "    \n",
    "    ax = axs[i]\n",
    "\n",
    "    clusters = modelcluster[0]\n",
    "    \n",
    "    im1 = ax.imshow(clusters.reshape(120, 65), cmap = cmap, aspect='auto', rasterized=True)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    ax.set_title(f\"{modelcluster[1]}\")\n",
    "\n",
    "    \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax1 = divider.append_axes('bottom', size='5%', pad=0.2)\n",
    "    cmap = ListedColormap(sns.color_palette(\"Spectral\", len(np.unique(clusters))).as_hex())\n",
    "    cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', label = \"Clusters\", pad=0.35)\n",
    "    bounds = ['', '2', '5','8']\n",
    "    #cbar.set_ticklabels(bounds)\n",
    "\n",
    "\n",
    "    #ax.set_title(f\"Variance explained: {explained_variance_score(data, data):.2f}\")\n",
    "\n",
    "#ax.set_xlabel('Component 1')\n",
    "#ax.set_ylabel('Component 2')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(savepath + \"clustering2\" + \".png\", bbox_inches=\"tight\", dpi=300, transparent=True,)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cmap = sns.diverging_palette(230, 30, as_cmap=True)\n",
    "#cmap = sns.color_palette(\"mako\", as_cmap=True)\n",
    "#cmap = sns.color_palette(\"magma\", as_cmap=True)\n",
    "#cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "#cmap = sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n",
    "cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "\n",
    "cmap2 = sns.color_palette(\"rocket\")\n",
    "cmap2 = sns.color_palette(\"ch:s=-.2,r=.6\", )\n",
    "#cmap2 = sns.color_palette(\"coolwarm\")\n",
    "cmap = 'RdYlBu'\n",
    "cmap = plt.get_cmap('tab10', len(np.unique(clusters)))\n",
    "\n",
    "cmap = ListedColormap(sns.color_palette(\"Spectral\", len(np.unique(clusters))).as_hex())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( dpi = 300, figsize = (2, 3))\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax1 = divider.append_axes('bottom', size='5%', pad=0.05)\n",
    "\n",
    "\n",
    "# im1 = ax[0].imshow(-u2[:, 0].reshape(120, 65), cmap = cmap, aspect='auto', )\n",
    "# im2 = ax[1].imshow(u2[:, 1].reshape(120, 65), cmap = cmap, aspect='auto', )\n",
    "\n",
    "#feature = np.sqrt(features[:, 0]**2 + features[:, 1]**2)\n",
    "\n",
    "im1 = ax.imshow(clusters.reshape(120, 65), cmap = cmap, aspect='auto', rasterized=True)\n",
    "\n",
    "# im1 = ax[0].imshow(features[:, 0].reshape(120, 65), cmap = cmap, aspect='auto', )\n",
    "# im2 = ax[1].imshow(features[:, 1].reshape(120, 65), cmap = cmap, aspect='auto', )\n",
    "\n",
    "\n",
    "\n",
    "cbar = fig.colorbar(im1, cax=cax1, orientation='horizontal', label = \"Clusters\", pad=0.35)\n",
    "\n",
    "\n",
    "#fig.colorbar(im2, cax=cax2, orientation='horizontal', label = \"low-dimensional space \\n (2nd component)\")\n",
    "\n",
    "#yticks = np.linspace(0, len(np.unique(clusters))+4, 3 )[:-1] +0.5\n",
    "\n",
    "\n",
    "#yticks[0] = 0.5\n",
    "#yticks[1] = 3.5\n",
    "#yticks[1] = 6.5\n",
    "\n",
    "#yticks += (yticks[1] - yticks[0]) / 2\n",
    "\n",
    "\n",
    "#cbar.set_ticks(yticks, labels=['LVZ',  'HVZ'])\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "#ax[1].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(savepath + f'cluster_best.pdf', transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,N_CLUSTERS , figsize = (N_CLUSTERS*2, 5))\n",
    "\n",
    "for i in range(N_CLUSTERS):\n",
    "    ax = axs[i]\n",
    "    im = ax.imshow(features[:, i].reshape(120, 65), aspect = 'auto', cmap = 'RdYlBu_r')\n",
    "    plt.colorbar(im, ax = ax, orientation = 'horizontal', label = 'ICA 1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5,4))\n",
    "\n",
    "im = ax.scatter(features[:, 0], features[:, 1], c = vs.reshape(-1), cmap = 'BrBG', s = 3, alpha = 0.75, rasterized=True)\n",
    "\n",
    "#plt.colorbar(im, ax = ax,  orientation = 'vertical', label = 'Shear wave \\n velocty (m/s)')\n",
    "ax.set_xlabel('ICA 1')\n",
    "ax.set_ylabel('ICA 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + \"PCA\" + \".pdf\", bbox_inches=\"tight\", dpi=300, transparent=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5,4))\n",
    "\n",
    "im = ax.scatter(features[:, 0], features[:, 1], c = vs.reshape(-1), cmap = 'RdBu', s = 3, alpha = 0.75, rasterized=True)\n",
    "\n",
    "plt.colorbar(im, ax = ax,  orientation = 'vertical', label = 'Shear wave \\n velocty (m/s)')\n",
    "ax.set_xlabel('ICA 1')\n",
    "ax.set_ylabel('ICA 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + \"PCA2\" + \".pdf\", bbox_inches=\"tight\", dpi=300, transparent=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cmap2 = sns.color_palette(\"hls\", 3)\n",
    "\n",
    "print(cmap2.as_hex())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    fit2 = umap.UMAP(densmap=False,\n",
    "        n_neighbors = 30, \n",
    "        min_dist = 0.,\n",
    "        n_components = 2,\n",
    "        random_state = 42,\n",
    "\n",
    "        )\n",
    "\n",
    "    u2 = fit2.fit_transform(\n",
    "        \n",
    "        #features[:,:],\n",
    "        cofficients[:,:],\n",
    "        #np.abs(c['sc']).reshape(7800,-1)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = {\"hspace\": 0.1, \"wspace\": 0.1, \"width_ratios\": [1, 1]}\n",
    "cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (6, 5), sharey=True, )\n",
    "\n",
    "ax = axs[0]\n",
    "gain = 3\n",
    "lnrwidth=10\n",
    "\n",
    "pcm = ax.imshow((u2[:,0].reshape(120, 65)), cmap = cmap, aspect='auto', rasterized=True,)\n",
    "\n",
    "fig.colorbar(pcm, ax=ax,  fraction=0.08, pad=0.1, orientation='horizontal', label = 'UMAP 1')\n",
    "#ax.contour(X, Y, vs, cmap = 'flag', alpha = 0.75 , linewidths = 0.75 )\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "pcm = ax.imshow((u2[:, 1].reshape(120, 65)), cmap = cmap, aspect='auto',\n",
    ")\n",
    "\n",
    "cbar = fig.colorbar(pcm, ax=ax,  fraction=0.08, pad=0.1, orientation='horizontal', label = 'UMAP 2')\n",
    "\n",
    "plt.savefig(savepath + \"umap__2d\" + \".pdf\", bbox_inches=\"tight\", dpi=300, transparent=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "im = ax.scatter(u2[:, 0], u2[:, 1], c = vs.reshape(-1), cmap = 'RdBu', s = 3, alpha = 0.75, rasterized=True)\n",
    "ax.title\n",
    "plt.colorbar(im, ax = ax,  orientation = 'vertical', label = 'Shear wave \\n velocty (m/s)')\n",
    "ax.set_xlabel('ICA 1')\n",
    "ax.set_ylabel('ICA 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(savepath + \"umap_test\" + \".pdf\", bbox_inches=\"tight\", dpi=300, transparent=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_localdim, axs_localdim = plt.subplots(1, )\n",
    "\n",
    "\n",
    "local_dims = umap.plot.diagnostic(fit2, diagnostic_type='local_dim', ax = axs_localdim, local_variance_threshold=0.7)\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(savepath+f'local_dim.png', bbox_inches=\"tight\", dpi=300 , transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=fig_localdim.get_axes()[0]\n",
    "\n",
    "x_r=ax.collections[0].get_offsets()[:,0]\n",
    "y_r=ax.collections[0].get_offsets()[:,1]\n",
    "\n",
    "z_r=ax.collections[0].get_array()\n",
    "\n",
    "plt.figure(dpi =300, figsize = (5, 3.5))\n",
    "\n",
    "plt.scatter(x_r, y_r, c = z_r, s = 0.5, cmap = 'RdBu')\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('V$_{s}$ (m/s)', rotation=270)\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "\n",
    "#plt.title(f'n_neighbors = {20}, min_dist = {min_dist}')\n",
    "\n",
    "plt.savefig(savepath+f'local_dimensionality.png', bbox_inches=\"tight\", dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ax=fig_localdim.get_axes()[0]\n",
    "\n",
    "x_r=ax.collections[0].get_offsets()[:,0]\n",
    "y_r=ax.collections[0].get_offsets()[:,1]\n",
    "\n",
    "z_r=ax.collections[0].get_array()\n",
    "\n",
    "fig2, axs = plt.subplots(1,1, figsize= (4, 3), dpi =300)\n",
    "\n",
    "pcm = axs.imshow(z_r.data.reshape(120, 65) , cmap = 'RdBu')\n",
    "\n",
    "#plt.contour(X, Y, vs, cmap = 'flag', alpha = 0.75 , linewidths = 0.75 )\n",
    "\n",
    "fig.colorbar(pcm, ax=axs,  fraction=0.08, pad=0.04)\n",
    "\n",
    "fig2.savefig(savepath+f'local_dim_model.png', bbox_inches=\"tight\", dpi=300 , transparent=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs = {\"hspace\": 0.1, \"wspace\": 0.1, \"width_ratios\": [1, 1]}\n",
    "cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize = (3, 4.5), sharey=True, )\n",
    "\n",
    "ax = axs\n",
    "\n",
    "gain = 3\n",
    "lnrwidth=10\n",
    "\n",
    "pcm = ax.imshow( z_r.data.reshape(120, 65) , cmap = 'RdBu', aspect='auto', rasterized=True,)\n",
    "\n",
    "fig.colorbar(pcm, ax=ax,  fraction=0.08, pad=0.1, orientation='horizontal', label = 'Local dimensionality')\n",
    "#ax.contour(X, Y, vs, cmap = 'flag', alpha = 0.75 , linewidths = 0.75 )\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(savepath+f'local_dim_model.pdf', bbox_inches=\"tight\", dpi=300 , transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_imag",
   "language": "python",
   "name": "test_imag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
